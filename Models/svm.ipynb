{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import utils.text_processing as util\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, make_scorer, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from scipy import sparse, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "OUTPUT_CLASSES = 22\n",
    "FILE_NAME = \"../Data/processed_data.csv\"\n",
    "TEST_SIZE_PERCENTAGE = 0.2\n",
    "CANDIDATES = [1, 10, 100]\n",
    "Y_LABEL_NAME = \"username\"\n",
    "TEXT_LABEL_NAME =\"raw_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\leobl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>username</th>\n",
       "      <th>syllables</th>\n",
       "      <th>periods</th>\n",
       "      <th>hyphens</th>\n",
       "      <th>commas</th>\n",
       "      <th>exclamations</th>\n",
       "      <th>questions</th>\n",
       "      <th>quotes</th>\n",
       "      <th>...</th>\n",
       "      <th>replies</th>\n",
       "      <th>retweets</th>\n",
       "      <th>links</th>\n",
       "      <th>smiles</th>\n",
       "      <th>bigsmiles</th>\n",
       "      <th>winks</th>\n",
       "      <th>bigwinks</th>\n",
       "      <th>unsures</th>\n",
       "      <th>semicolons</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Appreciate a pair of nice titties</td>\n",
       "      <td>0laotan</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>\"son</td>\n",
       "      <td>0laotan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>me nutting in her means no one else can get h...</td>\n",
       "      <td>0laotan</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>Her being a good person means she's entitled ...</td>\n",
       "      <td>0laotan</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>this. https://t.co/WiUKzhqXp1</td>\n",
       "      <td>0laotan</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           raw_text  username  \\\n",
       "0           4                 \"Appreciate a pair of nice titties   0laotan   \n",
       "1           6                                               \"son   0laotan   \n",
       "2          10   me nutting in her means no one else can get h...   0laotan   \n",
       "3          12   Her being a good person means she's entitled ...   0laotan   \n",
       "4          13                      this. https://t.co/WiUKzhqXp1   0laotan   \n",
       "\n",
       "   syllables  periods  hyphens  commas  exclamations  questions  quotes  ...  \\\n",
       "0         11        0        0       0             0          0       1  ...   \n",
       "1          1        0        0       0             0          0       1  ...   \n",
       "2         21        2        0       0             0          1       0  ...   \n",
       "3         24        1        0       0             0          1       0  ...   \n",
       "4          3        2        0       0             0          0       0  ...   \n",
       "\n",
       "   replies  retweets  links  smiles  bigsmiles  winks  bigwinks  unsures  \\\n",
       "0        0         0      0       0          0      0         0        0   \n",
       "1        0         0      0       0          0      0         0        0   \n",
       "2        0         0      1       0          0      0         0        0   \n",
       "3        0         0      1       0          0      0         0        0   \n",
       "4        0         0      1       0          0      0         0        0   \n",
       "\n",
       "   semicolons  hashtags  \n",
       "0           0         0  \n",
       "1           0         0  \n",
       "2           0         0  \n",
       "3           0         0  \n",
       "4           0         0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(FILE_NAME)\n",
    "util.down_nltk_stopwords()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get text and y label\n",
    "text = data[TEXT_LABEL_NAME]\n",
    "author = data[Y_LABEL_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, author_train, author_test = train_test_split(text, author, test_size = TEST_SIZE_PERCENTAGE, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@lumen wait is it realli', 'not even safe get educ anymor https://t.co/yhsotejuyz', '\"man camilla!soleil like', \"most peopl gonna wait jan. 2 start make differ 2016. i'm next year mode now. #letsgo #letsgetit #dreamchas\", 'hella stupid boy ðŸ˜…']\n"
     ]
    }
   ],
   "source": [
    "# Process data subsets\n",
    "processed_train = util.process_data(text_train)\n",
    "processed_test = util.process_data(text_test)\n",
    "\n",
    "print(processed_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  3144\n"
     ]
    }
   ],
   "source": [
    "# Create bag of words features\n",
    "## Fit Tfidf Vectorizer\n",
    "vectorizer = TfidfVectorizer(strip_accents = 'ascii', stop_words = 'english', min_df = 6)\n",
    "vectorizer.fit(processed_train)\n",
    "\n",
    "# Get size of vocabulary\n",
    "print('Vocabulary size: ', len(vectorizer.vocabulary_))\n",
    "\n",
    "# Create feature vectors\n",
    "words_train = vectorizer.transform(processed_train)\n",
    "words_test = vectorizer.transform(processed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid search object\n",
    "svm = SVC()\n",
    "params = {'kernel': ['linear'], 'C':[1, 10, 100]}\n",
    "scorer = make_scorer(accuracy_score)\n",
    "\n",
    "grid_obj = GridSearchCV(svm, params, scoring = scorer, verbose = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV 1/5; 1/3] START C=1, kernel=linear..........................................\n",
      "[CV 1/5; 1/3] END ...........C=1, kernel=linear;, score=0.639 total time=  14.8s\n",
      "[CV 2/5; 1/3] START C=1, kernel=linear..........................................\n",
      "[CV 2/5; 1/3] END ...........C=1, kernel=linear;, score=0.641 total time=  14.7s\n",
      "[CV 3/5; 1/3] START C=1, kernel=linear..........................................\n",
      "[CV 3/5; 1/3] END ...........C=1, kernel=linear;, score=0.638 total time=  14.7s\n",
      "[CV 4/5; 1/3] START C=1, kernel=linear..........................................\n",
      "[CV 4/5; 1/3] END ...........C=1, kernel=linear;, score=0.635 total time=  14.6s\n",
      "[CV 5/5; 1/3] START C=1, kernel=linear..........................................\n",
      "[CV 5/5; 1/3] END ...........C=1, kernel=linear;, score=0.622 total time=  14.8s\n",
      "[CV 1/5; 2/3] START C=10, kernel=linear.........................................\n",
      "[CV 1/5; 2/3] END ..........C=10, kernel=linear;, score=0.595 total time=  14.5s\n",
      "[CV 2/5; 2/3] START C=10, kernel=linear.........................................\n",
      "[CV 2/5; 2/3] END ..........C=10, kernel=linear;, score=0.598 total time=  14.4s\n",
      "[CV 3/5; 2/3] START C=10, kernel=linear.........................................\n",
      "[CV 3/5; 2/3] END ..........C=10, kernel=linear;, score=0.591 total time=  14.3s\n",
      "[CV 4/5; 2/3] START C=10, kernel=linear.........................................\n",
      "[CV 4/5; 2/3] END ..........C=10, kernel=linear;, score=0.589 total time=  14.4s\n",
      "[CV 5/5; 2/3] START C=10, kernel=linear.........................................\n",
      "[CV 5/5; 2/3] END ..........C=10, kernel=linear;, score=0.591 total time=  14.8s\n",
      "[CV 1/5; 3/3] START C=100, kernel=linear........................................\n",
      "[CV 1/5; 3/3] END .........C=100, kernel=linear;, score=0.565 total time=  17.7s\n",
      "[CV 2/5; 3/3] START C=100, kernel=linear........................................\n",
      "[CV 2/5; 3/3] END .........C=100, kernel=linear;, score=0.570 total time=  17.7s\n",
      "[CV 3/5; 3/3] START C=100, kernel=linear........................................\n",
      "[CV 3/5; 3/3] END .........C=100, kernel=linear;, score=0.571 total time=  17.2s\n",
      "[CV 4/5; 3/3] START C=100, kernel=linear........................................\n",
      "[CV 4/5; 3/3] END .........C=100, kernel=linear;, score=0.563 total time=  17.4s\n",
      "[CV 5/5; 3/3] START C=100, kernel=linear........................................\n",
      "[CV 5/5; 3/3] END .........C=100, kernel=linear;, score=0.565 total time=  18.1s\n"
     ]
    }
   ],
   "source": [
    "# Fit bag of words svm\n",
    "np.random.seed(6)\n",
    "word_svm = grid_obj.fit(words_train, author_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, kernel='linear')\n"
     ]
    }
   ],
   "source": [
    "print(word_svm.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([12.29427571, 12.15218687, 15.22364411]), 'std_fit_time': array([0.0453769 , 0.16381215, 0.32317975]), 'mean_score_time': array([2.53313718, 2.42423034, 2.50047569]), 'std_score_time': array([0.04389946, 0.03840066, 0.01934516]), 'param_C': masked_array(data=[1, 10, 100],\n",
      "             mask=[False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_kernel': masked_array(data=['linear', 'linear', 'linear'],\n",
      "             mask=[False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 1, 'kernel': 'linear'}, {'C': 10, 'kernel': 'linear'}, {'C': 100, 'kernel': 'linear'}], 'split0_test_score': array([0.63946188, 0.59484305, 0.56524664]), 'split1_test_score': array([0.64125561, 0.59820628, 0.56950673]), 'split2_test_score': array([0.63811659, 0.59147982, 0.5706278 ]), 'split3_test_score': array([0.63534425, 0.58914555, 0.56290648]), 'split4_test_score': array([0.62233685, 0.5913882 , 0.56492487]), 'mean_test_score': array([0.63530304, 0.59301258, 0.5666425 ]), 'std_test_score': array([0.00676415, 0.00317071, 0.00293063]), 'rank_test_score': array([1, 2, 3])}\n"
     ]
    }
   ],
   "source": [
    "print(word_svm.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6514798206278027\n",
      "Ave. Precision: 0.7022408047742655\n",
      "Ave. Recall: 0.6514798206278029\n",
      "Ave. F1 Score: 0.6592978441766544\n",
      "Training Time: 18.569517135620117 seconds\n",
      "Prediction Time: 3.989659547805786 seconds\n",
      "Confusion Matrix:\n",
      " [[ 22   1   3  31   0  16  19   3   1   0   0   0   3   0   7   0   2   0\n",
      "    0   1   7   4]\n",
      " [  0 234   1   4   0  14   2   4   0   0   0   0   2   0   7   0   0   2\n",
      "    0   0   2   0]\n",
      " [  2   0  60  14   0  36  14   6   1   0   0   0  12   0  11   0   0   5\n",
      "    0   0  13   7]\n",
      " [  2   1   5 147   1  42  18   5   0   0   0   0   8   0  16   0   5   1\n",
      "    0   0   8   2]\n",
      " [  1   2   1  26  10  13  15   2   0   0   0   0   5   0   4   0   2   1\n",
      "    0   2   1   4]\n",
      " [  0   2   9  39   2 141  27  13   2   0   0   0  17   0  30   0   0   3\n",
      "    0   1  16  10]\n",
      " [  0   1   2  34   0  48 421   4   1   0   0   0  14   0  10   0   0   6\n",
      "    0  10  19   4]\n",
      " [  0   1   8  40   1  58  16 119   0   0   0   0  12   0  15   0   0   2\n",
      "    0   0   4   4]\n",
      " [  4   2   3  13   0  24  11   1  79   0   0   0   4   0   7   0   1   0\n",
      "    0   0  11   1]\n",
      " [  0   0   0   2   0   3   0   1   1  35   0   0   3   0   3   0   0   0\n",
      "    0   0   1   0]\n",
      " [  0   0   0   0   0   0   0   2   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0   0   1   0   0   3   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   1]\n",
      " [  1   1   2  14   0  40  27   6   1   0   0   0 379   1  24   0   1   1\n",
      "    0   4  24   2]\n",
      " [  0   1   0   1   0   6  12   0   0   0   0   0   5  24   4   0   0   1\n",
      "    0   0   7   1]\n",
      " [  2   4   5  17   0  42  18   2   2   0   0   0  24   1 460   0   2   1\n",
      "    0   1  27   8]\n",
      " [  0   0   0   0   0   0   1   0   0   0   0   0   1   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  1   0   2  20   0  25   7   4   1   0   0   0  10   0  13   0 111   1\n",
      "    0   0  15   2]\n",
      " [  1   1   2  14   0  36  12   4   1   0   0   0   5   0   5   0   1 373\n",
      "    0   0   4   0]\n",
      " [  0   0   0   0   0   5   2   1   0   0   0   0   5   0   1   0   0   1\n",
      "   65   0   0   1]\n",
      " [  1   0   5  14   0  28  22   5   0   0   0   0  16   1  13   0   0   2\n",
      "    0 472   7   0]\n",
      " [  0   0   3   4   0  32  25   4   1   0   0   0  21   2  18   0   1   6\n",
      "    0   2 407   5]\n",
      " [  0   0  12   9   1  53  11   5   0   0   0   0   9   0   8   0   1   1\n",
      "    0   0   9  73]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Fit and evaluate Model 2 (Bag of words SVM)\n",
    "np.random.seed(28)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Fit model\n",
    "model2 = SVC(C = 1, kernel = 'linear')\n",
    "model2.fit(words_train, author_train)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "# Predict values for test set\n",
    "author_pred2 = model2.predict(words_test)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(author_test, author_pred2)\n",
    "precision, recall, f1, support = score(author_test, author_pred2)\n",
    "ave_precision = np.average(precision, weights = support/np.sum(support))\n",
    "ave_recall = np.average(recall, weights = support/np.sum(support))\n",
    "ave_f1 = np.average(f1, weights = support/np.sum(support))\n",
    "confusion = confusion_matrix(author_test, author_pred2, labels =  data[Y_LABEL_NAME].unique())\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Ave. Precision:\", ave_precision)\n",
    "print(\"Ave. Recall:\", ave_recall)\n",
    "print(\"Ave. F1 Score:\", ave_f1)\n",
    "print(\"Training Time:\", (t1 - t0), \"seconds\")\n",
    "print(\"Prediction Time:\", (t2 - t1), \"seconds\")\n",
    "print(\"Confusion Matrix:\\n\", confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 5\n",
      "Accuracy: [0.6403587443946188, 0.6414349775784753, 0.6376681614349776, 0.6376681614349776, 0.6398206278026906]\n",
      "Ave. Precision: [0.688486115848937, 0.689682217453364, 0.6816022483052767, 0.6853305337173741, 0.691684844720376]\n",
      "Ave. Recall: [0.640358744394619, 0.6414349775784755, 0.6376681614349777, 0.6376681614349777, 0.6398206278026908]\n",
      "Ave. F1 Score: [0.6466686600827178, 0.6482960247153159, 0.6379516056399197, 0.6444719897149782, 0.6464050860871541]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 5)\n",
    "\n",
    "accuracy_list = []\n",
    "prec_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "cnt = 0\n",
    "\n",
    "# Convert sparse matrix to array\n",
    "words_train_np = words_train.toarray()\n",
    "\n",
    "for train_inds, _ in kf.split(words_train):\n",
    "    cnt += 1\n",
    "    print('Run:', cnt)\n",
    "       \n",
    "    # Create data subsets\n",
    "    train_x = np.array([words_train_np[i] for i in train_inds])\n",
    "    train_y = [author_train.to_numpy()[i] for i in train_inds]\n",
    "    \n",
    "    # Convert train_x back to sparse matrix\n",
    "    train_x = sparse.csr_matrix(train_x)\n",
    "    \n",
    "    # Fit model\n",
    "    model2 = SVC(C = 1, kernel = 'linear')\n",
    "    model2.fit(train_x, train_y)\n",
    "\n",
    "\n",
    "    # Predict values for test set\n",
    "    author_pred2 = model2.predict(words_test)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(author_test, author_pred2)\n",
    "    precision, recall, f1, support = score(author_test, author_pred2)\n",
    "    ave_precision = np.average(precision, weights = support/np.sum(support))\n",
    "    ave_recall = np.average(recall, weights = support/np.sum(support))\n",
    "    ave_f1 = np.average(f1, weights = support/np.sum(support))\n",
    "    \n",
    "    accuracy_list.append(accuracy)\n",
    "    prec_list.append(ave_precision)\n",
    "    recall_list.append(ave_recall)\n",
    "    f1_list.append(ave_f1)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_list)\n",
    "print(\"Ave. Precision:\", prec_list)\n",
    "print(\"Ave. Recall:\", recall_list)\n",
    "print(\"Ave. F1 Score:\", f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_averages(true, pred, text):\n",
    "    \"\"\"Calculate average length of correctly and incorrectly classified examples\n",
    "    \n",
    "    Args:\n",
    "    true: list. List of correct labels.\n",
    "    pred: list. List of predicted labels.\n",
    "    text: list. List of text excerpts.\n",
    "    \n",
    "    Returns:\n",
    "    correct_ave_chars: float. Average length of correctly classified examples in characters.\n",
    "    incorrect_ave_chars: float. Average length of incorrectly classified examples in characters.\n",
    "    correct_ave_words: float. Average length of correctly classified examples in characters.\n",
    "    incorrect_ave_words: float. Average length of incorrectly classified examples in characters.\n",
    "    \"\"\"\n",
    "    \n",
    "    correct_len_chars = []\n",
    "    incorrect_len_chars = []\n",
    "    correct_len_words = []\n",
    "    incorrect_len_words = []\n",
    "\n",
    "    \n",
    "    for i in range(len(true)):\n",
    "        if true[i] == pred[i]:\n",
    "            correct_len_chars.append(len(text[i]))\n",
    "            correct_len_words.append(len(text[i].split()))\n",
    "        else:\n",
    "            incorrect_len_chars.append(len(text[i]))\n",
    "            incorrect_len_words.append(len(text[i].split()))\n",
    "    \n",
    "    correct_ave_chars = np.mean(correct_len_chars)\n",
    "    correct_ave_words = np.mean(correct_len_words)\n",
    "    incorrect_ave_chars = np.mean(incorrect_len_chars)\n",
    "    incorrect_ave_words = np.mean(incorrect_len_words)\n",
    "    \n",
    "    # Conduct two sample t-test\n",
    "    print('Character t-test')\n",
    "    print(stats.ttest_ind(correct_len_chars, incorrect_len_chars, equal_var = False))\n",
    "    \n",
    "    print('\\nWord t-test')\n",
    "    print(stats.ttest_ind(correct_len_words, incorrect_len_words, equal_var = False))\n",
    "    \n",
    "    return correct_ave_chars, correct_ave_words, incorrect_ave_chars, incorrect_ave_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character t-test\n",
      "Ttest_indResult(statistic=13.92926125327792, pvalue=3.2217212181006924e-43)\n",
      "\n",
      "Word t-test\n",
      "Ttest_indResult(statistic=9.893607634357823, pvalue=7.734767972916657e-23)\n"
     ]
    }
   ],
   "source": [
    "# Calculate averages for Model 2\n",
    "correct_ave_chars2, correct_ave_words2, incorrect_ave_chars2, incorrect_ave_words2 = calculate_averages(author_test.to_numpy(), author_pred2, text_test.to_numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
