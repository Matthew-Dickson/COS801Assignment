{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matthew-Dickson/COS801Assignment/blob/main/COS801Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUTeNwJGTf1w"
      },
      "source": [
        "<h2>Some usefull information:</h2>\n",
        "\n",
        "\n",
        "1.   https://www.kaggle.com/code/hsankesara/news-classification-using-han/notebook\n",
        "2.   https://medium.com/analytics-vidhya/hierarchical-attention-networks-d220318cf87e\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "8_rzz9fsdEf1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests, zipfile, io\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwLyfYZBT8Iv"
      },
      "source": [
        "<h2>Dataset section </h2>\n",
        "Datasets can be found here: https://archive.ics.uci.edu/ml/datasets.php"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "He6KF3ajT573"
      },
      "outputs": [],
      "source": [
        "zip_file_url =  'https://archive.ics.uci.edu/ml/machine-learning-databases/00454/dataset.zip' \n",
        "\n",
        "# Download data set and place in current directory under content directory\n",
        "r = requests.get(zip_file_url)\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall(\"./content\")\n",
        "#testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ulkZduvShsaF"
      },
      "outputs": [],
      "source": [
        "# Read training csv and place in panda data frame \n",
        "df = pd.read_csv(\n",
        "    './content/dataset/Gungor_2018_VictorianAuthorAttribution_data-train.csv',\n",
        "     encoding='latin-1'  \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2479
        },
        "id": "ZSnvDugJimqP",
        "outputId": "17594aef-a09f-4934-c3f7-65e8c7cd0ef9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ou have time to listen i will give you the ent...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wish for solitude he was twenty years of age a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>and the skirt blew in perfect freedom about th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>of san and the rows of shops opposite impresse...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>an hour s walk was as tiresome as three in a s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  author\n",
              "0  ou have time to listen i will give you the ent...       1\n",
              "1  wish for solitude he was twenty years of age a...       1\n",
              "2  and the skirt blew in perfect freedom about th...       1\n",
              "3  of san and the rows of shops opposite impresse...       1\n",
              "4  an hour s walk was as tiresome as three in a s...       1"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "48310\n",
            "5368\n"
          ]
        }
      ],
      "source": [
        "#Split data into train and test data (90% train data and 10% test data)\n",
        "#As required by assignment \n",
        "train_df, test_df = train_test_split(df, test_size=0.1)\n",
        "print(len(train_df))\n",
        "print(len(test_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Getting data for train and test\n",
        "X_train = train_df[\"text\"]\n",
        "X_test = test_df[\"text\"]\n",
        "\n",
        "#Getting labels for train and test\n",
        "y_train = train_df[\"author\"]\n",
        "y_test = test_df[\"author\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Data processing</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # Bag of Words: Term Frequencey and TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Initialize count vectorizer and \n",
        "fits the data based on word ngrams for instance ngram = 1\n",
        "will tokenize each individual word while ngram = 2 will\n",
        "tokenize every 2 words. This returns a Document-term matrix and the\n",
        "count vectorizer. Documentation can be found here:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html'''\n",
        "\n",
        "def initialise_term_frequency_vectorizer(data,ngram = 1):\n",
        "    vectorizer_tf = CountVectorizer(ngram_range=(ngram,ngram))\n",
        "    X = vectorizer_tf.fit_transform(data)\n",
        "    return X, vectorizer_tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Converts the vocabulary from word -> index to\n",
        "index -> word format'''\n",
        "def create_id2word_dictionary(vocabulary):\n",
        "    id2word_dictionary = {}\n",
        "    for key in vocabulary.keys():\n",
        "      id2word_dictionary[vocabulary[key]] = key\n",
        "    return id2word_dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "#initializing counter vectorizer inorder to get term frequency text representation\n",
        "X, vectorizer_tf = initialise_term_frequency_vectorizer(X_train,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2047067 1454522 3046033 ... 1591393 1591395 3437941]\n"
          ]
        }
      ],
      "source": [
        "#Get frequecy of each word/token\n",
        "token_counts = X.sum(axis=0)\n",
        "list_token_counts = token_counts.tolist()[0]\n",
        "#Sorting in descending order\n",
        "sorted_index = np.argsort(list_token_counts)[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['aa aa' 'aa ab' 'aa able' ... 'zest while' 'zest with' 'zest without']\n"
          ]
        }
      ],
      "source": [
        "#Check out the vocabulary generated by the counter vectorizer  \n",
        "print(vectorizer_tf.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "id2word_dictionary = create_id2word_dictionary(vectorizer_tf.vocabulary_)\n",
        "\n",
        "for index in sorted_index:\n",
        "    print(\"Highest occuring word:%s - count:%s\" % (id2word_dictionary[index], list_token_counts[index]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Model section</h2>\n",
        "<ol>\n",
        "<li> https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough \n",
        "</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Model creation\n",
        "NB: We need to find out how to create custome layers\"\"\"\n",
        "\n",
        "#This is just an example model taken from https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(3)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Calculate loss using specified model and loss function\n",
        "def loss(model, x, y, training = True,loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)):\n",
        "  # training=training is needed only if there are layers with different\n",
        "  # behavior during training versus inference (e.g. Dropout).\n",
        "  y_ = model(x, training=training)\n",
        "  return loss_function(y_true=y, y_pred=y_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Calculate loss values and gradients \n",
        "def calulate_loss_and_gradients(model, inputs, targets):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss_value = loss(model, inputs, targets, training=True)\n",
        "  return loss_value, tape.gradient(loss_value, model.trainable_variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Setting optimizer \n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Note: Rerunning this cell uses the same model parameters\n",
        "\n",
        "# Keep results for plotting\n",
        "train_loss_results = []\n",
        "train_accuracy_results = []\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  #Metrics for evaluation  \n",
        "  epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "  epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "  # Training loop - using batches of 32\n",
        "  for x, y in ds_train_batch:\n",
        "    # Optimize the model\n",
        "    loss_value, grads = calulate_loss_and_gradients(model, x, y)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Track progress\n",
        "    epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n",
        "    # Compare predicted label to actual label\n",
        "    # training=True is needed only if there are layers with different\n",
        "    # behavior during training versus inference (e.g. Dropout).\n",
        "    epoch_accuracy.update_state(y, model(x, training=True))\n",
        "\n",
        "  # End epoch\n",
        "  train_loss_results.append(epoch_loss_avg.result())\n",
        "  train_accuracy_results.append(epoch_accuracy.result())\n",
        "\n",
        "  if epoch % 50 == 0:\n",
        "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
        "                                                                epoch_loss_avg.result(),\n",
        "                                                                epoch_accuracy.result()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMbPzv+PhueaeacA2glZkkN",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "COS801Assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
