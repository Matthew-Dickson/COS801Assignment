{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matthew-Dickson/COS801Assignment/blob/main/COS801Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUTeNwJGTf1w"
      },
      "source": [
        "<h2>Some usefull information:</h2>\n",
        "\n",
        "\n",
        "1.   https://www.kaggle.com/code/hsankesara/news-classification-using-han/notebook\n",
        "2.   https://medium.com/analytics-vidhya/hierarchical-attention-networks-d220318cf87e\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "8_rzz9fsdEf1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests, zipfile, io\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwLyfYZBT8Iv"
      },
      "source": [
        "<h2>Dataset section </h2>\n",
        "Datasets can be found here: https://archive.ics.uci.edu/ml/datasets.php"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "He6KF3ajT573"
      },
      "outputs": [],
      "source": [
        "zip_file_url =  'https://archive.ics.uci.edu/ml/machine-learning-databases/00454/dataset.zip' \n",
        "\n",
        "# Download data set and place in current directory under content directory\n",
        "r = requests.get(zip_file_url)\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall(\"./content\")\n",
        "#testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ulkZduvShsaF"
      },
      "outputs": [],
      "source": [
        "# Read training csv and place in panda data frame \n",
        "df = pd.read_csv(\n",
        "    './content/dataset/Gungor_2018_VictorianAuthorAttribution_data-train.csv',\n",
        "     encoding='latin-1'  \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2479
        },
        "id": "ZSnvDugJimqP",
        "outputId": "17594aef-a09f-4934-c3f7-65e8c7cd0ef9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ou have time to listen i will give you the ent...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wish for solitude he was twenty years of age a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>and the skirt blew in perfect freedom about th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>of san and the rows of shops opposite impresse...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>an hour s walk was as tiresome as three in a s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  author\n",
              "0  ou have time to listen i will give you the ent...       1\n",
              "1  wish for solitude he was twenty years of age a...       1\n",
              "2  and the skirt blew in perfect freedom about th...       1\n",
              "3  of san and the rows of shops opposite impresse...       1\n",
              "4  an hour s walk was as tiresome as three in a s...       1"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "48310\n",
            "5368\n"
          ]
        }
      ],
      "source": [
        "#Split data into train and test data (90% train data and 10% test data)\n",
        "#As required by assignment \n",
        "train_df, test_df = train_test_split(df, test_size=0.1)\n",
        "print(len(train_df))\n",
        "print(len(test_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Getting data for train and test\n",
        "X_train = train_df[\"text\"]\n",
        "X_test = test_df[\"text\"]\n",
        "\n",
        "#Getting labels for train and test\n",
        "y_train = train_df[\"author\"]\n",
        "y_test = test_df[\"author\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Data processing</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # Bag of Words: Term Frequencey and TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Initialize count vectorizer and \n",
        "fits the data based on word ngrams for instance ngram = 1\n",
        "will tokenize each individual word while ngram = 2 will\n",
        "tokenize every 2 words. This returns a Document-term matrix and the\n",
        "count vectorizer. Documentation can be found here:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html'''\n",
        "\n",
        "def initialise_term_frequency_vectorizer(data,ngram = 1):\n",
        "    vectorizer_tf = CountVectorizer(ngram_range=(ngram,ngram))\n",
        "    X = vectorizer_tf.fit_transform(data)\n",
        "    return X, vectorizer_tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Converts the vocabulary from word -> index to\n",
        "index -> word format'''\n",
        "def create_id2word_dictionary(vocabulary):\n",
        "    id2word_dictionary = {}\n",
        "    for key in vocabulary.keys():\n",
        "      id2word_dictionary[vocabulary[key]] = key\n",
        "    return id2word_dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "#initializing counter vectorizer inorder to get term frequency text representation\n",
        "X, vectorizer_tf = initialise_term_frequency_vectorizer(X_train,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2047067 1454522 3046033 ... 1591393 1591395 3437941]\n"
          ]
        }
      ],
      "source": [
        "#Get frequecy of each word/token\n",
        "token_counts = X.sum(axis=0)\n",
        "list_token_counts = token_counts.tolist()[0]\n",
        "#Sorting in descending order\n",
        "sorted_index = np.argsort(list_token_counts)[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['aa aa' 'aa ab' 'aa able' ... 'zest while' 'zest with' 'zest without']\n"
          ]
        }
      ],
      "source": [
        "#Check out the vocabulary generated by the counter vectorizer  \n",
        "print(vectorizer_tf.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "id2word_dictionary = create_id2word_dictionary(vectorizer_tf.vocabulary_)\n",
        "\n",
        "for index in sorted_index:\n",
        "    print(\"Highest occuring word:%s - count:%s\" % (id2word_dictionary[index], list_token_counts[index]))\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMbPzv+PhueaeacA2glZkkN",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "COS801Assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
