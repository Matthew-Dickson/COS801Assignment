{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "import utils.text_processing as util\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from scipy import sparse, stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "OUTPUT_CLASSES = 22\n",
    "FILE_NAME = \"../Data/processed_data.csv\"\n",
    "TEST_SIZE_PERCENTAGE = 0.2\n",
    "Y_LABEL_NAME = \"username\"\n",
    "TEXT_LABEL_NAME =\"raw_text\"\n",
    "NUMBER_K_FOLDS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\leobl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(FILE_NAME)\n",
    "util.down_nltk_stopwords()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get text and y label\n",
    "text = data[TEXT_LABEL_NAME]\n",
    "author = data[Y_LABEL_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, author_train, author_test = train_test_split(text, author, test_size = TEST_SIZE_PERCENTAGE, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data subsets\n",
    "processed_train = util.process_data(text_train)\n",
    "processed_test = util.process_data(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  3144\n"
     ]
    }
   ],
   "source": [
    "# Create bag of words features\n",
    "## Fit Tfidf Vectorizer\n",
    "vectorizer = TfidfVectorizer(strip_accents = 'ascii', stop_words = 'english', min_df = 6)\n",
    "vectorizer.fit(processed_train)\n",
    "\n",
    "# Get size of vocabulary\n",
    "print('Vocabulary size: ', len(vectorizer.vocabulary_))\n",
    "\n",
    "# Create feature vectors\n",
    "words_train = vectorizer.transform(processed_train)\n",
    "words_test = vectorizer.transform(processed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 3\n",
      "Accuracy: [0.6059192825112107, 0.6019730941704036, 0.6019730941704036]\n",
      "Ave. Precision: [0.6495125484319788, 0.6458531115290364, 0.6477799180744149]\n",
      "Ave. Recall: [0.605919282511211, 0.6019730941704037, 0.6019730941704037]\n",
      "Ave. F1 Score: [0.6207107857393509, 0.6167052283679811, 0.6171412293046584]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(28)\n",
    "kf = KFold(n_splits = NUMBER_K_FOLDS)\n",
    "\n",
    "#Metrics \n",
    "test_accuracy_list = []\n",
    "prec_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "training_time_list = []\n",
    "prediction_time_list = []\n",
    "\n",
    "run = 0\n",
    "\n",
    "# Convert sparse matrix to array\n",
    "words_train_np = words_train.toarray()\n",
    "\n",
    "for train_inds, _ in kf.split(words_train):\n",
    "    run += 1\n",
    "    print('Run:', run)\n",
    "       \n",
    "    # Create data subsets\n",
    "    train_x = np.array([words_train_np[i] for i in train_inds])\n",
    "    train_y = [author_train.to_numpy()[i] for i in train_inds]\n",
    "    \n",
    "    # Convert train_x back to sparse matrix\n",
    "    train_x = sparse.csr_matrix(train_x)\n",
    "    \n",
    "    # Fit model\n",
    "    t0 = time.time()\n",
    "    model = SVC(C = 5, kernel = 'linear')\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    t1 = time.time()\n",
    "    # Predict values for test set\n",
    "    author_pred2 = model.predict(words_test)\n",
    "\n",
    "    t2 = time.time()\n",
    "\n",
    "    # Evaluate\n",
    "    test_accuracy = accuracy_score(author_test, author_pred2)\n",
    "    precision, recall, f1, support = score(author_test, author_pred2)\n",
    "    ave_precision = np.average(precision, weights = support/np.sum(support))\n",
    "    ave_recall = np.average(recall, weights = support/np.sum(support))\n",
    "    ave_f1 = np.average(f1, weights = support/np.sum(support))\n",
    "    training_time = (t1 - t0)\n",
    "    prediction_time = (t2 - t1)\n",
    "\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "    prec_list.append(ave_precision)\n",
    "    recall_list.append(ave_recall)\n",
    "    f1_list.append(ave_f1)\n",
    "    training_time_list.append(training_time)\n",
    "    prediction_time_list.append(prediction_time)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy_list)\n",
    "print(\"Ave. Precision:\", prec_list)\n",
    "print(\"Ave. Recall:\", recall_list)\n",
    "print(\"Ave. F1 Score:\", f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy across K-folds: 0.603288490284006\n",
      "Average Precision across K-folds: 0.6477151926784767\n",
      "Average Recall across K-folds: 0.603288490284006\n",
      "Average F1 Score across K-folds: 0.6181857478039968\n",
      "Average Training Time across K-folds: 8.776155948638916 seconds\n",
      "Prediction Time across K-folds: 2.631319046020508 seconds\n"
     ]
    }
   ],
   "source": [
    "#Get averages for kfold runs\n",
    "mean_accuracy_across_kfold = np.mean(test_accuracy_list)\n",
    "mean_percision_across_kfold = np.mean(prec_list)\n",
    "mean_recall_across_kfold = np.mean(recall_list)\n",
    "mean_f1_across_kfold = np.mean(f1_list)\n",
    "\n",
    "\n",
    "print(\"Average Accuracy across K-folds:\", mean_accuracy_across_kfold)\n",
    "print(\"Average Precision across K-folds:\", mean_percision_across_kfold)\n",
    "print(\"Average Recall across K-folds:\", mean_recall_across_kfold)\n",
    "print(\"Average F1 Score across K-folds:\", mean_f1_across_kfold)\n",
    "print(\"Average Training Time across K-folds:\", (t1 - t0), \"seconds\")\n",
    "print(\"Prediction Time across K-folds:\", (t2 - t1), \"seconds\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
